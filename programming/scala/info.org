* Scala
   :PROPERTIES:
   :ID:       295a3ef6-10e2-4faf-95ee-88bcbc248b92
   :BRAIN_PARENTS: 67069e3b-0693-4cd6-8429-949de721e47e
   :END:

#+BEGIN_description 
Functional programming and category theory on the JVM.
#+END_description 

A JVM language with strong foundations in category theory. Used often for distributed data-intensive applications with frameworks like Akka, Storm, and Spark. 
* Apache Spark

#+BEGIN_description 
Distributed, fault-tolerant data processing for data pipelines and large-scale data science.
#+END_description 

* Scala Deck :deck: 
** first n                                                             :note:
   :PROPERTIES:
   :ANKI_NOTE_TYPE: Basic
   :ANKI_NOTE_ID: 1517330349906
   :END:
*** Front
    get first n elements of sequence in scala
*** Back
#+begin_src scala  
seq.take(n)
#+end_src
** concat sequences                                                    :note:
   :PROPERTIES:
   :ANKI_NOTE_TYPE: Basic
   :ANKI_NOTE_ID: 1517330350091
   :END:
*** Front
    combine two sequences in Scala
*** Back
#+begin_src scala 
seq1 ++ seq2
#+end_src

** string from seq scala                                               :note:
   :PROPERTIES: 
   :ANKI_NOTE_TYPE: Basic
   :ANKI_NOTE_ID: 1517330350305
   :END: 
*** Front
join a sequence with a delimiter in scala

*** Back
~seq.mkString(delim)~
** scala elementwise comparison                                        :note:
   :PROPERTIES: 
   :ANKI_NOTE_TYPE: Basic
   :ANKI_NOTE_ID: 1517330350490
   :END: 
*** Front
get the element-wise comparison of two sequences in scala
*** Back
~seq1.deep == seq2.deep~
** scala nested loop                                                   :note:
   :PROPERTIES: 
   :ANKI_NOTE_TYPE: Basic
   :ANKI_NOTE_ID: 1517330350709
   :END: 
*** Front
construct a nested loop in scala
*** Back
~for (x <- xs; y <- ys) {}~
** create a list by repeating in scala                                 :note:
   :PROPERTIES: 
   :ANKI_NOTE_TYPE: Basic
   :ANKI_NOTE_ID: 1517330350893
   :END: 
*** Front
Repeat item i n times to form a list in scala
*** Back
#+begin_src scala
List.fill(n)(i)
#+end_src 
** get the current timestamp in scala                                  :note:
   :PROPERTIES: 
   :ANKI_NOTE_TYPE: Basic
   :ANKI_NOTE_ID: 1517330351133
   :END: 
*** Front
Get the current timestamp in Scala (and Java)
*** Back
#+begin_src scala
LocalDateTime.of(Calendar.getInstance.getTime.toInstant, ZoneId.of("America/New York"))
#+end_src  
It has to be a new calendar instance!

* Spark Deck                                                           :deck: 
** udf on list of columns                                              :note:
   :PROPERTIES:
   :ANKI_NOTE_TYPE: Basic
   :ANKI_NOTE_ID: 1517330296324
   :END:
*** Front
    apply a udf to a list of columns in spark
*** Back
#+begin_src scala 
var tempdf = df
cols.map{c => tempdf.withColumn(c, udf(col(c)))}
#+end_src 
** select column sequence                                              :note:
   :PROPERTIES:
   :ANKI_NOTE_TYPE: Basic
   :ANKI_NOTE_ID: 1517330296492
   :END:
*** Front
select a sequence of column names in Spark
*** Back
#+begin_src scala 
df.select(cols.head, cols.tail: _*)
#+end_src

** basic model                                                         :note:
   :PROPERTIES:
   :ANKI_NOTE_TYPE: Basic
   :ANKI_NOTE_ID: 1517330100262
   :END:

*** Front
High-level summary of any Spark program

*** Back
1. represent data as collection of RDDs
2. build up a series of lazy transformations on those RDDs
3. perform actions to get the results of those transformations
** spark row to sequence                                               :note:
   :PROPERTIES: 
   :ANKI_NOTE_TYPE: Basic
   :ANKI_NOTE_ID: 1517330296951
   :END: 
*** Front
Convert a Row to a sequence in Spark
*** Back
#+begin_src scala
Row.toSeq 
#+end_src
** spark join by row number :note:
:PROPERTIES:
:ANKI_NOTE_TYPE: Basic
:ANKI_NOTE_ID: 1517936166278
:END:

*** Front

Get the row number of a dataframe in spark

*** Back

#+begin_src scala 
df.rdd.zipWithIndex
#+end_src 

** count null values in a spark column :note:
:PROPERTIES:
:ANKI_NOTE_TYPE: Basic
:ANKI_NOTE_ID: 1517936166520
:END:

*** Front

Find the count of null values in column "x" of a DataFrame df

*** Back

#+begin_src scala
df.filter(df("x").isNull).count()
#+end_src

** filter out the NaN values in a Spark dataframe :note:
:PROPERTIES:
:ANKI_NOTE_TYPE: Basic
:ANKI_NOTE_ID: 1517936166738
:END:

*** Front

Filter the NaN values out of column y in a Spark DataFrame

*** Back

#+begin_src scala 
val filteredDF = df.filter(!df("y").isNaN)
#+end_src 

** spark drop all nulls :note:
:PROPERTIES:
:ANKI_NOTE_TYPE: Basic
:ANKI_NOTE_ID: 1517936166950
:END:

*** Front

Get rid of any rows with null or NaN values from a df in Spark

*** Back

#+begin_src scala
df.na.drop()
#+end_src

** spark chi-squared two vectors                                       :note:
   :PROPERTIES: 
   :ANKI_NOTE_TYPE: Basic
   :CREATED: <2018-02-06 Tue>
   :ANKI_NOTE_ID: 1518118509892
   :END: 
*** Front
Compare the distributions of two vectors using a chi-squared test in Spark
*** Back
#+begin_src scala 
Statistics.chiSqTest(vec1, vec2)
#+end_src
